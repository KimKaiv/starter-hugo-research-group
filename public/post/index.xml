<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Latest Posts | CRUCIAL</title>
    <link>https://CRUCIALab.net/post/</link>
      <atom:link href="https://CRUCIALab.net/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Latest Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Thu, 10 Mar 2022 18:41:05 +0100</lastBuildDate>
    <image>
      <url>https://CRUCIALab.net/media/icon_hued9da3865e5e2275fc30e6cd1353f7b7_2285_512x512_fill_lanczos_center_3.png</url>
      <title>Latest Posts</title>
      <link>https://CRUCIALab.net/post/</link>
    </image>
    
    <item>
      <title>Why no climate warranties?</title>
      <link>https://CRUCIALab.net/post/why-no-climate-warranties/</link>
      <pubDate>Thu, 10 Mar 2022 18:41:05 +0100</pubDate>
      <guid>https://CRUCIALab.net/post/why-no-climate-warranties/</guid>
      <description>&lt;p&gt;The market for climate risk forecasts suffers from a problem economists call 
“asymmetric information”: the sellers of forecasts probably know more about their 
accuracy than the buyers. A short or medium range weather forecast can be verified 
in a couple of weeks, and reasonable statistics about their accuracy can be 
accumulated in a matter of months but, because a climate forecast might not verify 
for years or decades, buyers cannot usually use a forecaster’s track record to assess 
their accuracy. A market in which the quality of a product won’t be apparent for years 
is susceptible to “adverse selection”, which is economists’ way of saying it will be 
a magnet for charlatans. This creates a challenge for providers of climate forecasts 
who are competent and have invested in research and development to improve the accuracy: 
how can they persuade buyers that &lt;em&gt;they&lt;/em&gt; are not charlatans?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;&amp;ldquo;Talk is cheap.&amp;rdquo;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;Climate service providers can emphasise their credentials and highlight the scientific 
effort that goes into their products; they might allow users to see how their models 
work &amp;mdash; although many users don’t have the expertise to assess the models, and if the 
seller uses proprietary models this might not be option anyway. The problem with these 
solutions is that “talk is cheap”, and it is difficult for a buyer to distinguish 
authentic expertise from slick marketing.&lt;/p&gt;
&lt;p&gt;The economist George Akerlof, who won a Nobel Prize for his work on information 
asymmetries, explained the problem in a &lt;a href=&#34;https://www.jstor.org/stable/1879431&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;classic paper published in 1970&lt;/a&gt; which 
described the market for used cars. As with climate forecasts, it is difficult 
for the buyer of a used car to know its quality and, also like climate forecasts, 
a car is an infrequent purchase so there is little opportunity to accumulate much 
first-hand knowledge about the reliability of a seller. These attributes made the 
market for used cars attractive to less-than-reputable characters, creating the 
stereotype of the untrustworthy used-car dealer.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“Dealers offered warranties to differentiate themselves from competitors.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The reputation of used-car dealers has improved since the 1970s, partly because 
the sector adopted one of the strategies for dealing with asymmetric information 
described by Akerlof: offering cash back warranties. In the U.K. used-car 
warranties became mandatory in 2015, but even before then, dealers offered 
warranties to differentiate themselves from competitors and enable them to 
charge higher prices.&lt;/p&gt;
&lt;p&gt;A promise of a refund if a product fails can serve two distinct purposes: 
indemnifying the buyer for losses caused by the failure and providing a credible 
signal that the seller believes failure is unlikely. An inaccurate climate 
forecast may cause the user a loss far above the price they paid for the forecast: 
for example, they may build sea defences that are too low, or too high. While a 
forecast provider might not be able to indemnify the user against all costs, 
they can still offer a refund to signal their confidence in the accuracy of the 
forecast. The credibility of the signal is determined, not by whether the refund 
will compensate the user, but whether it will be painful for the provider.&lt;/p&gt;
&lt;p&gt;Climate forecasts, generally, are not categorically right or wrong: they are 
probabilistic forecasts — or they should be — in which probabilities are assigned 
to specified outcomes. How could the terms of a refund be structured for this type 
of product? There is a literature on this subject going back 70 years, when &lt;a href=&#34;https://journals.ametsoc.org/view/journals/mwre/78/1/1520-0493_1950_078_0001_vofeit_2_0_co_2.xml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Glenn 
Brier introduced a quadratic scoring rule for assessing probability forecasts&lt;/a&gt; and 
&lt;a href=&#34;https://www.jstor.org/stable/2984087&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;I.J. Good suggested the logarithmic scoring rule as a way to reward forecasters.&lt;/a&gt; 
Since then more work has been done on the concept of &lt;a href=&#34;https://sites.stat.washington.edu/raftery/Research/PDF/Gneiting2007jasa.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;“proper scoring rules”&lt;/a&gt; for 
incentivizing forecasters. More recently, proper scoring rules have been &lt;a href=&#34;https://www.jstor.org/stable/43189651&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;suggested 
as the basis for incentives that screen the views of informed and uninformed experts 
when only a single forecast is being made.&lt;/a&gt; There are subtleties to be appreciated, 
and pitfalls to avoid, but in principle a proper scoring rule could be used to 
evaluate the quality of a probabilistic forecast and determine the refund to which 
the buyer is entitled. This can only be done when the actual outcome becomes known, 
which in some cases may be more than twenty years after the forecast is made. Some 
people might think that providing a warranty on a timescale of decades is 
unrealistic but other industries do it: e.g., 10-year structural warranties for 
new houses or the lifetime warranties offered by high-end manufacturers that can 
be claimed decades after a product is purchased. It is also analogous to deferring 
bonuses in the finance industry. Climate service providers unwilling to expose 
themselves to some risk on climatic timescales are arguably in the wrong business.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;“A well-structured warranty can be used to communicate how much confidence a forecast provider has.”&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The information asymmetry in the market for climate forecasts is at least as bad 
as the one for used cars, so why aren’t warranties commonplace? There is no need 
for a regulator to force forecasters to provide warranties. As with used-cars, 
any provider is free to unilaterally offer warranties to differentiate themselves. 
One possibility is that few climate service providers have enough confidence in 
their forecasts to give warranties. However, a probabilistic forecast combined 
with a well-structured warranty can be used to communicate how much (or little) 
confidence a forecast provider has, and this is information that the forecast user 
should want. An alternative explanation is that there is little demand from buyers 
for sellers to provide a credible signal of their confidence in forecast accuracy. 
This interpretation is consistent with the view that buyers of climate risk 
information use it to satisfy regulators, auditors, or other stakeholders, but 
aren’t really worried about its accuracy; they just need it to give them “plausible 
deniability”. Worse than indifference, is the possibility that some organisations 
&lt;em&gt;want&lt;/em&gt; inaccurate forecasts. Back in the early 2000s, the issuers of mortgage-backed 
securities didn’t want accurate credit ratings for them, they wanted top-notch 
ratings, even if these did not reflect the true risk of default. These issuers would 
play rating agencies off against each other to get their desired ratings, and the 
ensuing proliferation of mis-rated securities contributed to the subprime crisis of 
2007/08.&lt;/p&gt;
&lt;p&gt;End-users of climate risk forecasts who care about accuracy should signal their 
commitment by being prepared to pay more for forecasts that come with a warranty. 
This will motivate reputable providers to offer such guarantees, thus differentiating 
themselves from less able providers. Without a strategy to combat asymmetric 
information about climate forecasts it is possible that climate service providers 
will gradually acquire the reputation of 1970s used-car dealers.&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;This article originally appeared on Medium as &lt;a href=&#34;https://medium.com/@Mark_Roulston/why-no-climate-warranties-c32abe91b181&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Why no climate warranties?&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
</description>
    </item>
    
    <item>
      <title>Cost witout benefit</title>
      <link>https://CRUCIALab.net/post/cost-without-benefit/</link>
      <pubDate>Fri, 17 Dec 2021 18:41:05 +0100</pubDate>
      <guid>https://CRUCIALab.net/post/cost-without-benefit/</guid>
      <description>&lt;p&gt;In 1970, the economist George Akerlof published what would become a classic paper called, 
“&lt;a href=&#34;https://www.jstor.org/stable/1879431&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;The Market for Lemons: Quality Uncertainty and Market Mechanisms.&lt;/a&gt;” The “lemons” he was 
referring to were low-quality used cars. His insight was that, in markets where buyers 
cannot evaluate products, they will not pay a premium for purported high quality. This, 
in turn, will give sellers little incentive to provide high quality products. Consequently, 
the average quality of products in the market declines, further reducing the amount buyers 
are willing to pay. In economics jargon an “information asymmetry” between buyers and sellers 
results in “adverse selection”. The process identified by Akerlof is now a textbook example 
of market failure.&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;A buyer of short-range weather forecasts does not need to know about meteorology to evaluate them.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The used car market is not the only one susceptible to the “lemon” problem. (Ironically, in the market for actual lemons, buyers can easily assess freshness and taste). A rapidly growing market that is also vulnerable is the one for information about long-range climate risks. A buyer of short-range weather forecasts does not need to know about meteorology to evaluate them. They just need to know some statistics and be able to collect a sufficiently large sample of forecasts and subsequent outcomes to estimate the skill of the provider. For forecasts 1–2 days ahead, this sample might be obtained in a matter of months. However, companies increasingly need forecasts of climate-related risks on horizons of years, and even decades. Obviously, on these time scales, waiting to see how forecasts turn out before choosing a provider is not practical. Some providers want to keep their models and methods proprietary, but even with full transparency it can be hard for non-specialists to judge their quality. When a market has these characteristics, buyers need to be alert to the danger that bad products will crowd out good ones.&lt;/p&gt;
&lt;p&gt;There are theoretical reasons why the quality of climate information may deteriorate, but is 
there evidence of this happening in practice? In 2019, an &lt;a href=&#34;https://www.science.org/doi/10.1126/science.aay8442&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;article in Science&lt;/a&gt; suggested that 
some climate information products might be “&amp;hellip;operating outside of the bounds of scientific merit” 
and, if you work in the field of climate risk, you have probably heard stories of providers 
pushing precision and granularity beyond what is scientifically plausible. But is there systematic 
evidence that some climate information has a lemon flavour?&lt;/p&gt;
&lt;p&gt;Shortcomings in long-range climate-related predictions might not become apparent for years but there are already indications of problems. Hain et al. recently looked at six different metrics of firm-level exposure to physical climate risks. Three of the measures, from TruCost (part of S&amp;amp;P), Carbon4 Finance and South Pole, were based on the output of climate models, while the other three, from Truvalue Labs (part of FactSet) and two academic groups were constructed by analysing the content of social media, earnings calls and 10-K filings. The correlations between the metrics were generally low, both overall and within specific sectors. Indeed, after allowing for the multiplicity of correlations tested, there were arguably no significant correlations between different metrics. Think about this for a moment: six metrics all claiming to measure the same thing, or at least to be proxies for the same thing, yet they don’t correlate with each other. Even the most generous interpretation implies that most of the metrics contain little information about the common factor they claim to measure. Perhaps one of the metrics is a good measure, but which one? Perhaps none of the metrics are informative. Consider an analogy in which there are six different blood tests to measure the same substance, but when hundreds of subjects are given all six tests there is no correlation between the results. Would a doctor base clinical decisions on any of these tests? Would a medical regulator allow such a situation?&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;By the time the accuracy becomes apparent all those involved are likely to have moved on.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The problem of information asymmetry between buyers and sellers might be compounded when the buyers of climate information are not concerned with the accuracy of the information either. Much of the demand for forward-looking climate information is driven by regulators and shareholders expecting firms to disclose their climate-related risks. The exercise is one of “putting numbers in boxes”, and by the time the accuracy of the numbers becomes apparent, all those involved are likely to have moved on. This is not conducive to accurate disclosure.&lt;/p&gt;
&lt;p&gt;At this point, you might protest that there are many people involved in climate-risk who want to provide accurate and rigorous information that is supported by science, and there are many people who want such information to make accurate disclosures of risk to shareholders. This is true, but it misses the point: the problems in the market for climate-risk information are structural. The used-car market did not become a market for lemons because it was populated by untrustworthy salesmen; it attracted and created the stereotype of the used-car salesman because of its intrinsic information asymmetries.&lt;/p&gt;
&lt;p&gt;If information asymmetry and the potential for adverse selection in the market for forward-looking climate information is not addressed, it is likely that more evidence will accumulate that undermines the credibility of climate-related risk disclosures. This will make mandatory disclosures a cost to businesses without any offsetting benefit to society.&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;This article originally appeared on Medium as &lt;a href=&#34;https://medium.com/@Mark_Roulston/how-climate-related-risk-disclosures-could-become-a-cost-without-a-benefit-e74085062e31&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How climate-related risk disclosures could become a cost without a benefit&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
</description>
    </item>
    
    <item>
      <title>How good are AGORA forecasts?</title>
      <link>https://CRUCIALab.net/post/how-good-are-agora-forecasts/</link>
      <pubDate>Tue, 24 Nov 2020 18:41:05 +0100</pubDate>
      <guid>https://CRUCIALab.net/post/how-good-are-agora-forecasts/</guid>
      <description>&lt;p&gt;&lt;strong&gt;“Calibration means the frequency of events matches their predicted probabilities”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;There are two desirable characteristics that probability forecasts should have: 
&lt;em&gt;calibration&lt;/em&gt; and &lt;em&gt;sharpness&lt;/em&gt;. Calibration means that the frequency of events matches 
their predicted probabilities while sharpness refers to how often predicted probabilities 
are close to 0% or 100% for binary forecasts or how narrow the forecast distribution is 
for a numerical value. Making sharp forecasts is trivial if you don’t care about 
calibration, so in this blog we will investigate whether forecasts produced by AGORA 
are calibrated.&lt;/p&gt;
&lt;p&gt;If I predict a 90% chance of an event occurring and it doesn’t happen then I am not 
wrong because I allowed for a 10% chance of it not happening. If, however, I repeatedly 
predict a 90% chance of things occurring and they keep failing to happen you will lose 
confidence in my ability as a forecaster. This is because my forecasts are not well
calibrated. Calibration means the frequency of events matches their predicted 
probabilities: for example, if you record all the times I said there was a 60% 
chance of something occurring, that something should have happened 60% of those times.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-1-an-illustration-of-the-percentiles-of-a-predicted-probability-distribution-if-the-predicted-distribution-is-well-calibrated-there-is-a-5-chance-the-actual-value-will-fall-below-the-5th-percentile-a-25-chance-it-will-fall-below-the-25th-percentile-etc&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Normal distribution&#34; srcset=&#34;
               /post/how-good-are-agora-forecasts/normald_hu13e4b697727ee04d039eeb86276590db_40642_fc503aa7bd30259b584e0a144a443119.webp 400w,
               /post/how-good-are-agora-forecasts/normald_hu13e4b697727ee04d039eeb86276590db_40642_f62d7ff834d5460c18e741deda1c933b.webp 760w,
               /post/how-good-are-agora-forecasts/normald_hu13e4b697727ee04d039eeb86276590db_40642_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://CRUCIALab.net/post/how-good-are-agora-forecasts/normald_hu13e4b697727ee04d039eeb86276590db_40642_fc503aa7bd30259b584e0a144a443119.webp&#34;
               width=&#34;760&#34;
               height=&#34;632&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Figure 1&lt;/strong&gt;: An illustration of the percentiles of a predicted probability distribution. If the predicted distribution is well calibrated there is a 5% chance the actual value will fall below the 5th percentile, a 25% chance it will fall below the 25th percentile etc.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;For well calibrated forecasts of numerical values half the time the actual value 
should fall below the median (50th percentile) of the predicted distribution and 
half the time it should fall above it. Similarly, the actual value should fall 
below the 10th percentile of the predicted distribution in 10 per cent of cases, 
below the 20th percentile in 20 per cent of cases, and so on. These percentiles 
are illustrated in Figure 1. If we plot the percentage of forecasts for which the
actual value falls below a given percentile of the forecast distribution we should 
get a diagonal line. Because we have a finite number of forecasts there will be 
deviations from this line, even if the forecasts are perfectly calibrated, with 
the size of the deviations shrinking as the number of forecasts used in the analysis 
increases.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-2-a-calibration-plot-for-23-agora-probabilistic-at-a-normalized-lead-time-of-075-after-25-of-the-time-between-opening-and-settlement-has-elapsed-the-grey-envelope-shows-the-amount-of-deviation-from-the-diagonal-that-can-be-expected-if-the-23-forecasts-were-perfectly-calibrated-the-plot-is-consistent-with-agora-forecasts-being-well-calibrated&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;0.75 lead-time calibration plot&#34; srcset=&#34;
               /post/how-good-are-agora-forecasts/calibration_lead75_hu0a8c8bbdc336901a73d82ceab9f87aa1_199466_08efc9f35ac03b8a8b2260125e2bc06c.webp 400w,
               /post/how-good-are-agora-forecasts/calibration_lead75_hu0a8c8bbdc336901a73d82ceab9f87aa1_199466_cba2f8ed7de98267aee30bcb80bb29eb.webp 760w,
               /post/how-good-are-agora-forecasts/calibration_lead75_hu0a8c8bbdc336901a73d82ceab9f87aa1_199466_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://CRUCIALab.net/post/how-good-are-agora-forecasts/calibration_lead75_hu0a8c8bbdc336901a73d82ceab9f87aa1_199466_08efc9f35ac03b8a8b2260125e2bc06c.webp&#34;
               width=&#34;760&#34;
               height=&#34;759&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Figure 2&lt;/strong&gt;: A calibration plot for 23 AGORA probabilistic at a normalized lead time of 0.75 (after 25% of the time between opening and settlement has elapsed). The grey envelope shows the amount of deviation from the diagonal that can be expected if the 23 forecasts were perfectly calibrated. The plot is consistent with AGORA forecasts being well calibrated.
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;When performing a calibration analysis we need to choose a forecast lead time. If 
all the forecasts in the analysis are of the same type then this is straightforward: 
for example, we might want to check the calibration of rainfall forecasts 3 days 
before the target date. If, however, we are dealing with different types of forecasts, 
choosing a common lead time might not make sense: Harold Wilson said that, “A week 
is a long time in politics”, but a week is no time at all in climate prediction. We 
will express all lead times relative to the duration of the prediction market: A 
normalized lead time of 0.25 means that a quarter of the time between opening and 
settlement remains until settlement.&lt;/p&gt;
&lt;p&gt;Figure 2 shows the calibration of 23 AGORA forecasts. The forecasts included in 
the analysis were all the public markets that have been run on the AGORA platform 
with cash incentives for participants. This includes predictions of monthly UK 
temperature, values of an El Niño-Southern Oscillation index and monthly totals 
of COVID-19 cases in the U.S. The yellow line follows the diagonal with some 
deviations but the deviations lie within the grey envelope, which indicates how 
much deviation would occur with 23 perfectly calibrated forecasts.&lt;/p&gt;
&lt;p&gt;Figures 3 and 4 show the calibration plot for normalized lead time of 0.50 and 
0.25. The plot for 0.25 suggests the frequency of the actual value falling below 
the median is only about 30% although this deviation is still consistent with 
calibrated forecasts due to the relatively small number of forecasts in the analysis.&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-3-as-figure-2-but-for-a-normalized-lead-time-of-05&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;0.50 lead-time calibration plot&#34; srcset=&#34;
               /post/how-good-are-agora-forecasts/calibration_lead50_hu31b9b0587f2695c03240d66e69d5aef4_27727_9e1685bca80c36af472c9cf6ba8c8d47.webp 400w,
               /post/how-good-are-agora-forecasts/calibration_lead50_hu31b9b0587f2695c03240d66e69d5aef4_27727_1ab5b4c3a7081951e3e912026d63ccb7.webp 760w,
               /post/how-good-are-agora-forecasts/calibration_lead50_hu31b9b0587f2695c03240d66e69d5aef4_27727_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://CRUCIALab.net/post/how-good-are-agora-forecasts/calibration_lead50_hu31b9b0587f2695c03240d66e69d5aef4_27727_9e1685bca80c36af472c9cf6ba8c8d47.webp&#34;
               width=&#34;760&#34;
               height=&#34;759&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Figure 3&lt;/strong&gt;: As figure 2 but for a normalized lead time of 0.5
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  id=&#34;figure-figure-4-as-figure-2-but-for-a-normalized-lead-time-of-025&#34;&gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;0.25 lead-time calibration plot&#34; srcset=&#34;
               /post/how-good-are-agora-forecasts/calibration_lead25_hu2c02e0b230d2949f0f23e4c5afed84ac_27605_6954c7242f4ae369301b6edeab448e6f.webp 400w,
               /post/how-good-are-agora-forecasts/calibration_lead25_hu2c02e0b230d2949f0f23e4c5afed84ac_27605_016c76393c339ca7faba61fd8344d5e8.webp 760w,
               /post/how-good-are-agora-forecasts/calibration_lead25_hu2c02e0b230d2949f0f23e4c5afed84ac_27605_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://CRUCIALab.net/post/how-good-are-agora-forecasts/calibration_lead25_hu2c02e0b230d2949f0f23e4c5afed84ac_27605_6954c7242f4ae369301b6edeab448e6f.webp&#34;
               width=&#34;760&#34;
               height=&#34;759&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;figcaption&gt;
      &lt;strong&gt;Figure 4&lt;/strong&gt;: As figure 2 but for a normalized lead time of 0.25
    &lt;/figcaption&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;“AGORA forecasts are consistent with being well calibrated.”&lt;/strong&gt;&lt;/p&gt;
&lt;p&gt;These results show the AGORA forecasts are consistent with being well calibrated 
so it is not unreasonable to take the probabilities that AGORA produces at face 
value. Prescriptions of how to make rational decisions in the face of uncertainty 
often assume that quantitative probabilities can be assigned to potential outcomes. 
AGORA prediction markets can be used as a source of these probabilities.&lt;/p&gt;
&lt;br&gt;
&lt;p&gt;This article originally appeared on Medium as &lt;a href=&#34;https://medium.com/hvmd/how-good-are-agora-forecasts-8065bf372067&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;How good are AGORA forecasts?&lt;/a&gt;&lt;/p&gt;
&lt;br&gt;
</description>
    </item>
    
  </channel>
</rss>
